{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qn0aZcjZRsu",
        "outputId": "bf356106-8fdd-47ad-e6cd-c65c96005f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "7/7 [==============================] - 1s 113ms/step\n",
            "7/7 [==============================] - 1s 94ms/step\n",
            "Wasserstein Distance: 0.3960235728754258\n",
            "Jensen-Shannon Divergence: 0.45850980281829834\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.preprocessing import image as keras_image\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Mount Google Drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        try:\n",
        "            img = keras_image.load_img(img_path, target_size=(200,200))  # VGG16 expects images of size 224x224\n",
        "            img_data = keras_image.img_to_array(img)\n",
        "            img_data = np.expand_dims(img_data, axis=0)\n",
        "            img_data = preprocess_input(img_data)\n",
        "            images.append(img_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping file {img_path} due to error: {e}\")\n",
        "    return np.vstack(images)\n",
        "\n",
        "\n",
        "# Specify the paths to your folders in Google Drive\n",
        "folder_path_original = '/content/drive/MyDrive/Healthy/Healthy'\n",
        "folder_path_generated = '/content/drive/MyDrive/hi/Healthy_510'\n",
        "\n",
        "# Load the pre-trained VGG16 model\n",
        "model = VGG16(weights='imagenet', include_top=False)\n",
        "\n",
        "# Load original and generated images\n",
        "original_images = load_images_from_folder(folder_path_original)\n",
        "generated_images = load_images_from_folder(folder_path_generated)\n",
        "\n",
        "# Ensure that the number of original and generated images is the same\n",
        "min_length = min(len(original_images), len(generated_images))\n",
        "original_images = original_images[:min_length]\n",
        "generated_images = generated_images[:min_length]\n",
        "\n",
        "\n",
        "# Ensure that the number of original and generated images is the same\n",
        "assert len(original_images) == len(generated_images), \"The number of original and generated images must be the same.\"\n",
        "\n",
        "# Get the embeddings for all images\n",
        "original_embeddings = model.predict(original_images)\n",
        "generated_embeddings = model.predict(generated_images)\n",
        "\n",
        "# Flatten the embeddings to 1D\n",
        "original_embeddings_1D = original_embeddings.flatten()\n",
        "generated_embeddings_1D = generated_embeddings.flatten()\n",
        "\n",
        "# Normalize the embeddings to [0, 1] range\n",
        "scaler = MinMaxScaler()\n",
        "original_embeddings_1D = scaler.fit_transform(original_embeddings_1D.reshape(-1, 1)).flatten()\n",
        "generated_embeddings_1D = scaler.transform(generated_embeddings_1D.reshape(-1, 1)).flatten()\n",
        "\n",
        "from scipy.stats import wasserstein_distance, entropy\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Flatten the embeddings to 1D\n",
        "original_embeddings_1D = original_embeddings.flatten()\n",
        "generated_embeddings_1D = generated_embeddings.flatten()\n",
        "\n",
        "# Calculate the Wasserstein distance\n",
        "wasserstein_dist = wasserstein_distance(original_embeddings_1D, generated_embeddings_1D)\n",
        "\n",
        "# Calculate the Jensen-Shannon divergence\n",
        "def jensen_shannon_divergence(p, q):\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (entropy(p, m) + entropy(q, m))\n",
        "\n",
        "jensen_shannon_div = jensen_shannon_divergence(original_embeddings_1D, generated_embeddings_1D)\n",
        "\n",
        "print(f'Wasserstein Distance: {wasserstein_dist}')\n",
        "print(f'Jensen-Shannon Divergence: {jensen_shannon_div}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import jensenshannon\n",
        "from scipy.linalg import sqrtm\n",
        "import numpy as np\n",
        "\n",
        "# Calculate the Jensen-Shannon divergence\n",
        "def jensen_shannon_divergence(p, q):\n",
        "    m = 0.5 * (p + q)\n",
        "    return 0.5 * (entropy(p, m) + entropy(q, m))\n",
        "\n",
        "# Normalize the embeddings to make them into probability distributions\n",
        "generated_embeddings_normalized = generated_embeddings / (np.sum(generated_embeddings, axis=1, keepdims=True) + 1e-10)  # Add a small constant to avoid division by zero\n",
        "\n",
        "# Calculate the pairwise Jensen-Shannon divergences\n",
        "js_distances = np.zeros((len(generated_embeddings_normalized), len(generated_embeddings_normalized)))\n",
        "\n",
        "for i in range(len(generated_embeddings_normalized)):\n",
        "    for j in range(len(generated_embeddings_normalized)):\n",
        "        js_distances[i, j] = jensen_shannon_divergence(generated_embeddings_normalized[i].flatten(), generated_embeddings_normalized[j].flatten())\n",
        "\n",
        "# Calculate the minimum and maximum observed diversity scores\n",
        "min_val = np.min(js_distances)\n",
        "max_val = np.max(js_distances)\n",
        "\n",
        "diversity_score = np.mean(js_distances)\n",
        "# Normalize the diversity score\n",
        "normalized_diversity_score = (diversity_score - min_val) / (max_val - min_val)\n",
        "\n",
        "print(normalized_diversity_score )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pBuo1RTZrEx",
        "outputId": "5534be3f-88e3-425a-ec83-79b1e23359fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8174721904650105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RzN01SU9fHOO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}